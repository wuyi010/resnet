{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13fAxneRvTGu1vTrdsFNQjnXyWQRb56v-",
      "authorship_tag": "ABX9TyMNKyoCjhZBqMo1ecP200Ah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuyi010/resnet/blob/main/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1、搭建resnet-layer模型\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RGMXdpIrRI5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dube4JbkeE5W",
        "outputId": "530ca579-a290-49f8-da7a-710e6c8089c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Pytorch官方ResNet模型\\nfrom torchvision.models import resnet34\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "# 1、搭建resnet-layer模型\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torchvision.models.resnet\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# 构建ResNet-34模型\n",
        "def resnet34(num_classes=1000, include_top=True):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "# 构建ResNet-50模型\n",
        "def resnet50(num_classes=1000, include_top=True):\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "\n",
        "\n",
        "# net = resnet34()# 模型网络结构可视化\n",
        "# print(net)\n",
        "\n",
        "\"\"\"\n",
        "# 1. 使用torchsummary中的summary查看模型的输入输出形状、顺序结构，网络参数量，网络模型大小等信息\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = net.to(device)\n",
        "summary(model, (3, 224, 224))    # 3是RGB通道数，即表示输入224 * 224的3通道的数据\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# 2. 使用torchviz中的make_dot生成模型的网络结构，pdf图包括计算路径、网络各层的权重、偏移量\n",
        "from torchviz import make_dot\n",
        "\n",
        "X = torch.rand(size=(1, 3, 224, 224))    # 3是RGB通道数，即表示输入224 * 224的3通道的数据\n",
        "Y = net(X)\n",
        "vise = make_dot(Y, params=dict(net.named_parameters()))\n",
        "vise.view()\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Pytorch官方ResNet模型\n",
        "from torchvision.models import resnet34\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2、划分训练集和验证集,训练集train和验证集val"
      ],
      "metadata": {
        "id": "1FB24-rmFUNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1、准备数据集文件夹\n",
        "\n",
        "   #data_root = \"/content/drive/MyDrive/ColabNotebooks/data\"\n",
        "\n",
        "\n",
        "#4、划分训练集和验证集,训练集train和验证集val\n",
        "import os\n",
        "from shutil import copy, rmtree\n",
        "import random\n",
        "\n",
        "\n",
        "def mk_file(file_path: str):\n",
        "    if os.path.exists(file_path):\n",
        "        # 如果文件夹存在，则先删除原文件夹在重新创建\n",
        "        rmtree(file_path)\n",
        "    os.makedirs(file_path)\n",
        "\n",
        "def split_train_val():  \n",
        "    random.seed(0)# 保证随机可复现    \n",
        "    split_rate = 0.1# 将数据集中10%的数据划分到验证集中\n",
        "\n",
        "    # 指向你解压后的flower_photos文件夹\n",
        "\n",
        "    # cwd = os.getcwd()\n",
        "    # data_root = os.path.join(cwd, \"flower_data\")\n",
        "    data_root = \"/content/drive/MyDrive/ColabNotebooks/data\"\n",
        "    origin_flower_path = os.path.join(data_root, \"flower_photos\")\n",
        "    assert os.path.exists(origin_flower_path), \"path '{}' does not exist.\".format(origin_flower_path)\n",
        "\n",
        "    flower_class = [cla for cla in os.listdir(origin_flower_path)\n",
        "               if os.path.isdir(os.path.join(origin_flower_path, cla))]\n",
        "\n",
        "    # 建立保存训练集的文件夹\n",
        "    train_root = os.path.join(data_root, \"train\")\n",
        "    mk_file(train_root)\n",
        "    for cla in flower_class:\n",
        "        # 建立每个类别对应的文件夹\n",
        "        mk_file(os.path.join(train_root, cla))\n",
        "\n",
        "    # 建立保存验证集的文件夹\n",
        "    val_root = os.path.join(data_root, \"val\")\n",
        "    mk_file(val_root)\n",
        "    for cla in flower_class:\n",
        "        # 建立每个类别对应的文件夹\n",
        "        mk_file(os.path.join(val_root, cla))\n",
        "\n",
        "    for cla in flower_class:\n",
        "        cla_path = os.path.join(origin_flower_path, cla)\n",
        "        images = os.listdir(cla_path)\n",
        "        num = len(images)\n",
        "        # 随机采样验证集的索引\n",
        "        eval_index = random.sample(images, k=int(num*split_rate))\n",
        "        for index, image in enumerate(images):\n",
        "            if image in eval_index:\n",
        "                # 将分配至验证集中的文件复制到相应目录\n",
        "                image_path = os.path.join(cla_path, image)\n",
        "                new_path = os.path.join(val_root, cla)\n",
        "                copy(image_path, new_path)\n",
        "            else:\n",
        "                # 将分配至训练集中的文件复制到相应目录\n",
        "                image_path = os.path.join(cla_path, image)\n",
        "                new_path = os.path.join(train_root, cla)\n",
        "                copy(image_path, new_path)\n",
        "            print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n",
        "        print()\n",
        "\n",
        "    print(\"processing done!\")\n",
        "\n",
        "if __name__ == 'split_train_val':\n",
        "    split_train_val\n",
        "\n",
        "\"\"\"\n",
        "flower_class = [cla for cla in os.listdir(origin_flower_path) \n",
        "   if os.path.isdir(os.path.join(origin_flower_path, cla))]\n",
        "这段代码是一个列表推导式，用于获取给定目录 origin_flower_path 下的子目录，\n",
        "并将它们存储在列表 flower_class 中。\n",
        "下面对代码进行解释：\n",
        "os.listdir(origin_flower_path): os.listdir() 函数返回指定目录下的所有文件和文件夹的名称列表，其中 origin_flower_path 是要遍历的目录路径。\n",
        "os.path.isdir(os.path.join(origin_flower_path, cla)): os.path.isdir() 函数用于检查给定路径是否为一个目录。os.path.join(origin_flower_path, cla) 将目录路径origin_flower_path和文件夹名称cla连接起来形成完整的子目录路径，然后通过os.path.isdir()来判断该路径是否为一个目录。\n",
        "[cla for cla in os.listdir(origin_flower_path) if os.path.isdir(os.path.join(origin_flower_path, cla))]: 这是一个列表推导式，用于遍历 os.listdir(origin_flower_path) 返回的文件和文件夹名称列表。对于列表中的每个元素 cla，通过 os.path.isdir(os.path.join(origin_flower_path, cla)) 判断其是否为一个目录。如果是目录，则将其添加到列表 flower_class 中。\n",
        "最终，flower_class 列表将包含 origin_flower_path 目录下的所有子目录（即花朵的类别名称）。这个列表可以用于构建花朵分类任务的标签或类别列表，以便在模型训练或预测时进行使用。\n",
        "\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2hT5IuBYeq",
        "outputId": "ef5453aa-bc6b-4997-b3cb-118a5129baa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sunflowers] processing [699/699]\n",
            "[tulips] processing [799/799]\n",
            "[daisy] processing [633/633]\n",
            "[dandelion] processing [898/898]\n",
            "[roses] processing [641/641]\n",
            "processing done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3、调整ResNet-34模型结构，迁移学习数据集flower_photos\n"
      ],
      "metadata": {
        "id": "3N99S99nvF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#训练\n",
        "# !pip install python==3.6\n",
        "# !pip install torch==1.10.0 torchvision==0.11.1\n",
        "# !pip install tensorflow==2.4.1\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "# from models import resnet34\n",
        "\n",
        "# 导入 ResNet-34 模型\n",
        "import torchvision.models as models\n",
        "net = models.resnet34(pretrained=True)\n",
        "def main():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"using {} device.\".format(device))\n",
        "#2、数据预处理\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                       transforms.RandomHorizontalFlip(),\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "        \"val\": transforms.Compose([transforms.Resize(256),\n",
        "                      transforms.CenterCrop(224),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "\n",
        "    # data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
        "    # image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
        "    \n",
        "    image_path = \"/content/drive/MyDrive/ColabNotebooks/data\"\n",
        "    assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
        "\n",
        "#3、创建数据集实例   \n",
        "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
        "                         transform=data_transform[\"train\"])\n",
        "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
        "                         transform=data_transform[\"val\"])\n",
        "    \n",
        "     \n",
        "    \"\"\"\n",
        "    cla_dict 字典对象的内容将被转换为JSON格式的字符串，并写入到名为\n",
        "    class_indices.json的文件中。这个文件可以用于存储类别索引或标签的映射关系，\n",
        "    以便在模型训练或预测时进行使用\n",
        "    \"\"\"\n",
        "    flower_list = train_dataset.class_to_idx # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}\n",
        "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
        "    # write dict into json file\n",
        "    json_str = json.dumps(cla_dict, indent=4)\n",
        "    with open('class_indices.json', 'w') as json_file:\n",
        "        json_file.write(json_str)\n",
        "    \n",
        "    \n",
        "#5、创建数据加载器\n",
        "    batch_size = 16\n",
        "    # number of workers = min{系统的CPU核心数量,batch_size的值，8}\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  \n",
        "    print('Using {} dataloader workers every process'.format(nw))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                           batch_size=batch_size, \n",
        "                           shuffle=True,\n",
        "                           num_workers=nw)\n",
        "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
        "                            batch_size=batch_size, \n",
        "                            shuffle=False,\n",
        "                            num_workers=nw)\n",
        "    \n",
        "    train_num = len(train_dataset)\n",
        "    val_num = len(validate_dataset)\n",
        "    print(\"using {} images for training, {} images for validation.\" .format(train_num, val_num))\n",
        "    \n",
        "\n",
        "\n",
        "# 2、定义ResNet34模型，使用预训练模型\n",
        "    #net = resnet34()\n",
        "    # 加载预训练模型的权重参数到网络模型中\n",
        "    # load pretrain weights， url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
        "    #model_weight_path = \"./resnet34-pre.pth\" \n",
        "    model_weight_path = \"/content/drive/MyDrive/ColabNotebooks/weightsFiles/resnet34-0.pth\"\n",
        "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
        "    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))\n",
        "    # for param in net.parameters():\n",
        "    #     param.requires_grad = False #关闭梯度计算\n",
        "\n",
        "    # change fc layer structure  #调整模型结构\n",
        "    in_channel = net.fc.in_features #全连接层的输入通道\n",
        "    net.fc = nn.Linear(in_channel, 5)\n",
        "    net.to(device)\n",
        "#3、定义损失函数，交叉熵损失函数\n",
        "    # define loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "#4.定义优化器Adam\n",
        "    # construct an optimizer： \n",
        "    #列表推导式从网络模型的所有参数中筛选出需要进行梯度更新的参数\n",
        "    \n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params, lr=0.0001)\n",
        "\n",
        "    epochs = 10\n",
        "    best_acc = 0.0\n",
        "    #save_path = './resNet34.pth' \n",
        "    save_path = '/content/drive/MyDrive/ColabNotebooks/weightsFiles/resnet34-1.pth'\n",
        "    train_steps = len(train_loader)\n",
        "    for epoch in range(epochs):\n",
        "        # train\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(images.to(device))\n",
        "            loss = loss_function(logits, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch+1,epochs,loss)\n",
        "\n",
        "        # validate\n",
        "        net.eval() #关闭梯度计算\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
        "            for val_data in val_bar:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = net(val_images.to(device))\n",
        "                # loss = loss_function(outputs, test_labels)\n",
        "                predict_y = torch.max(outputs, dim=1)[1]\n",
        "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
        "\n",
        "                val_bar.desc = \"valid epoch[{}/{}]\".format(epoch+1,epochs)\n",
        "\n",
        "        val_accurate = acc / val_num\n",
        "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
        "              (epoch+1, running_loss / train_steps, val_accurate))\n",
        "\n",
        "        if val_accurate > best_acc:\n",
        "            best_acc = val_accurate\n",
        "            torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6CPODt37MD",
        "outputId": "e8a149f1-d83d-457f-95a3-42f6c03aaf12"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cpu device.\n",
            "Using 2 dataloader workers every process\n",
            "using 3306 images for training, 364 images for validation.\n",
            "train epoch[1/10] loss:0.767: 100%|██████████| 207/207 [28:26<00:00,  8.25s/it]\n",
            "valid epoch[1/10]: 100%|██████████| 23/23 [00:59<00:00,  2.59s/it]\n",
            "[epoch 1] train_loss: 0.504  val_accuracy: 0.882\n",
            "train epoch[2/10] loss:0.214: 100%|██████████| 207/207 [27:54<00:00,  8.09s/it]\n",
            "valid epoch[2/10]: 100%|██████████| 23/23 [00:59<00:00,  2.59s/it]\n",
            "[epoch 2] train_loss: 0.325  val_accuracy: 0.931\n",
            "train epoch[3/10] loss:0.528: 100%|██████████| 207/207 [27:55<00:00,  8.09s/it]\n",
            "valid epoch[3/10]: 100%|██████████| 23/23 [01:04<00:00,  2.81s/it]\n",
            "[epoch 3] train_loss: 0.292  val_accuracy: 0.940\n",
            "train epoch[4/10] loss:0.182: 100%|██████████| 207/207 [27:49<00:00,  8.07s/it]\n",
            "valid epoch[4/10]: 100%|██████████| 23/23 [00:59<00:00,  2.60s/it]\n",
            "[epoch 4] train_loss: 0.254  val_accuracy: 0.942\n",
            "train epoch[5/10] loss:0.407: 100%|██████████| 207/207 [27:47<00:00,  8.05s/it]\n",
            "valid epoch[5/10]: 100%|██████████| 23/23 [01:00<00:00,  2.65s/it]\n",
            "[epoch 5] train_loss: 0.250  val_accuracy: 0.918\n",
            "train epoch[6/10] loss:0.384: 100%|██████████| 207/207 [27:52<00:00,  8.08s/it]\n",
            "valid epoch[6/10]: 100%|██████████| 23/23 [00:58<00:00,  2.55s/it]\n",
            "[epoch 6] train_loss: 0.234  val_accuracy: 0.937\n",
            "train epoch[7/10] loss:0.341: 100%|██████████| 207/207 [27:51<00:00,  8.08s/it]\n",
            "valid epoch[7/10]: 100%|██████████| 23/23 [00:59<00:00,  2.58s/it]\n",
            "[epoch 7] train_loss: 0.230  val_accuracy: 0.940\n",
            "train epoch[8/10] loss:0.458: 100%|██████████| 207/207 [28:01<00:00,  8.13s/it]\n",
            "valid epoch[8/10]: 100%|██████████| 23/23 [00:59<00:00,  2.58s/it]\n",
            "[epoch 8] train_loss: 0.204  val_accuracy: 0.937\n",
            "train epoch[9/10] loss:0.048: 100%|██████████| 207/207 [28:05<00:00,  8.14s/it]\n",
            "valid epoch[9/10]: 100%|██████████| 23/23 [00:59<00:00,  2.57s/it]\n",
            "[epoch 9] train_loss: 0.195  val_accuracy: 0.920\n",
            "train epoch[10/10] loss:0.433: 100%|██████████| 207/207 [28:06<00:00,  8.15s/it]\n",
            "valid epoch[10/10]: 100%|██████████| 23/23 [01:00<00:00,  2.63s/it]\n",
            "[epoch 10] train_loss: 0.184  val_accuracy: 0.948\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4、resnet34 + CIFAR10数据集训练"
      ],
      "metadata": {
        "id": "Cc0yi11RZOES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "训练resnet34 + CIFAR10数据集\n",
        "# 搭建resnet-layer模型\n",
        "#\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torchvision.models.resnet\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# 构建ResNet-34模型\n",
        "def resnet34(num_classes=1000, include_top=True):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn,optim,tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import  make_grid\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "\n",
        "#全局变量\n",
        "batch_size=256   #每次喂入的数据量\n",
        "\n",
        "# num_print=int(50000//batch_size//4)\n",
        "num_print=100\n",
        "\n",
        "epoch_num=70 #总迭代次数\n",
        "\n",
        "lr=0.01\n",
        "step_size=10  #每n次epoch更新一次学习率\n",
        "\n",
        "#数据获取(数据增强,归一化)\n",
        "def transforms_RandomHorizontalFlip():\n",
        "\n",
        "    #transforms.Compose(),将一系列的transforms有序组合,实现按照这些方法依次对图像操作\n",
        "\n",
        "    #ToTensor()使图片数据转换为tensor张量,这个过程包含了归一化,图像数据从0~255压缩到0~1,这个函数必须在Normalize之前使用\n",
        "    #实现原理,即针对不同类型进行处理,原理即各值除以255,\n",
        "    #最后通过torch.from_numpy将PIL Image或者 numpy.ndarray()针对具体类型转成torch.tensor()数据类型\n",
        "\n",
        "    #Normalize()是归一化过程,ToTensor()的作用是将图像数据转换为(0,1)之间的张量,Normalize()则使用公式(x-mean)/std\n",
        "    #将每个元素分布到(-1,1). 归一化后数据转为标准格式,\n",
        "    transform_train=transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n",
        "\n",
        "\n",
        "    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.485,0.456,0.406),(0.226,0.224,0.225))])\n",
        "\n",
        "\n",
        "    #root:cifar-10 的根目录,data_path\n",
        "    #train:True=训练集, False=测试集\n",
        "    #transform:(可调用,可选)-接收PIL图像并返回转换版本的函数\n",
        "    #download:true=从互联网上下载数据,并将其放在root目录下,如果数据集已经下载,就什么都不干\n",
        "    train_dataset=datasets.CIFAR10(root='../../data_hub/cifar10/data_1',train=True,transform=transform_train,download=True)\n",
        "    test_dataset=datasets.CIFAR10(root='../../data_hub/cifar10/data_1',train=False,transform=transform,download=True)\n",
        "\n",
        "    return train_dataset,test_dataset\n",
        "\n",
        "#数据增强:随机翻转\n",
        "train_dataset,test_dataset=transforms_RandomHorizontalFlip()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#Dataloader(....)\n",
        "dataset:就是pytorch已有的数据读取接口,或者自定义的数据接口的输出,该输出要么是torch.utils.data.Dataset类的对象,\n",
        "要么是继承自torch.utils.data.Dataset类的自定义类的对象\n",
        "\n",
        "batch_size:如果有50000张训练集,则相当于把训练集平均分成(50000/batch_size)份,每份batch_size张图片\n",
        "train_loader中的每个元素相当于一个分组,一个组中batch_size图片,\n",
        "\n",
        "shuffle:设置为True时会在每个epoch重新打乱数据(默认:False),一般在训练数据中会采用\n",
        "num_workers:这个参数必须>=0,0的话表示数据导入在主进程中进行,其他大于0的数表示通过多个进程来导入数据,可以加快数据导入速度\n",
        "drop_last:设定为True如果数据集大小不能被批量大小整除的时候,将丢到最后一个不完整的batch(默认为False)\n",
        "'''\n",
        "\n",
        "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "#模型,优化器\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#from VggNet import *\n",
        "\n",
        "model=resnet34().to(device)\n",
        "\n",
        "#在多分类情况下一般使用交叉熵\n",
        "# torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。\n",
        "# 预测的概率大于1不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。\n",
        "# 在实际训练中，如果做的是分类任务，且使用CrossEntropyLoss作为损失函数的话，\n",
        "# 神经网络的部分就没必要加入nn.Softmax或者nn.LogSoftmax等之类的，因为在CrossEntropyLoss已经内置了该功能。\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "'''\n",
        "params(iterable)-待优化参数的iterable或者定义了参数组的dict\n",
        "lr(float):学习率\n",
        "\n",
        "momentum(float)-动量因子\n",
        "\n",
        "weight_decay(float):权重衰减,使用的目的是防止过拟合.在损失函数中,weight decay是放在正则项前面的一个系数,正则项一般指示模型的复杂度\n",
        "所以weight decay的作用是调节模型复杂度对损失函数的影响,若weight decay很大,则复杂的模型损失函数的值也就大.\n",
        "\n",
        "dampening:动量的有抑制因子\n",
        "\n",
        "optimizer.param_group:是长度为2的list,其中的元素是两个字典\n",
        "optimzer.param_group:长度为6的字典,包括['amsgrad','params','lr','weight_decay',eps']\n",
        "optimzer.param_group:表示优化器状态的一个字典\n",
        "\n",
        "'''\n",
        "optimizer=optim.SGD(model.parameters(),lr=lr,momentum=0.8,weight_decay=0.001) #神经网络优化器\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "scheduler 调整学习率而设置，我这里设置的gamma衰减率为0.5，step_size为10，也就是每10个epoch将学习率衰减至原来的0.5倍。\n",
        "optimizer(Optimizer):要更改学习率的优化器\n",
        "milestones(list):递增的list,存放要更新的lr的epoch\n",
        "gamma:(float):更新lr的乘法因子\n",
        "last_epoch:：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。\n",
        "默认为-1表示从头开始训练，即从epoch=1\n",
        "'''\n",
        "schedule=optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=0.5,last_epoch=-1)\n",
        "\n",
        "\n",
        "\n",
        "#训练\n",
        "loss_list=[]  #为了后续画出损失图\n",
        "start=time.time()\n",
        "\n",
        "#train\n",
        "for epoch in range(epoch_num):\n",
        "    ww = 0\n",
        "    running_loss=0.0\n",
        "    #0是对i的给值(循环次数从0开始计数还是从1开始计数的问题):\n",
        "    #???\n",
        "    for i,(inputs,labels) in enumerate(train_loader,0):\n",
        "\n",
        "        #将数据从train_loader中读出来,一次读取的样本是32个\n",
        "        inputs,labels=inputs.to(device),labels.to(device)\n",
        "\n",
        "        #用于梯度清零,在每次应用新的梯度时,要把原来的梯度清零,否则梯度会累加\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "\n",
        "        loss=criterion(outputs,labels).to(device)\n",
        "\n",
        "        #反向传播,pytorch会自动计算反向传播的值\n",
        "        loss.backward()\n",
        "        #对反向传播以后对目标函数进行优化\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        if(i+1)%num_print==0:\n",
        "            print('[%d epoch,%d]  loss:%.6f' %(epoch+1,i+1,running_loss/num_print))\n",
        "            running_loss=0.0\n",
        "\n",
        "    lr_1=optimizer.param_groups[0]['lr'] #返回优化器的第一个参数组的学习率\n",
        "    print(\"learn_rate:%.15f\"%lr_1)\n",
        "    schedule.step() #10 per epoch\n",
        "\n",
        "end=time.time()\n",
        "print(\"time:{}\".format(end-start))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#测试\n",
        "\n",
        "#由于训练集不需要梯度更新,于是进入测试模式\n",
        "model.eval()\n",
        "correct=0.0\n",
        "total=0\n",
        "with torch.no_grad(): #训练集不需要反向传播\n",
        "    print(\"=======================test=======================\")\n",
        "    for inputs,labels in test_loader:\n",
        "        inputs,labels=inputs.to(device),labels.to(device)\n",
        "        outputs=model(inputs)\n",
        "\n",
        "        pred=outputs.argmax(dim=1)  #返回每一行中最大值元素索引\n",
        "        total+=inputs.size(0)    #输入张量的第一维度的大小\n",
        "        correct+=torch.eq(pred,labels).sum().item() #sum()返回张量中所有元素的总和，item()返回标量张量的值\n",
        "\n",
        "print(\"Accuracy of the network on the 10000 test images:%.2f %%\" %(100*correct/total) )\n",
        "print(\"===============================================\")\n",
        "\n",
        "# # PATH = './content/drive/MyDrive/Colab Notebooks/model'  \n",
        "PATH = '/content/drive/MyDrive/ColabNotebooks/modelSave/ResNet34'\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "torch.save({\n",
        "       'epoch': epoch,\n",
        "       'model_state_dict': model.state_dict(),\n",
        "       'optimizer_state_dict': optimizer.state_dict(),\n",
        "       'loss': loss,\n",
        "      \n",
        "      }, PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc2sKMTatCs7",
        "outputId": "914de2d1-a408-43d9-e4a1-0299eda1b075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data_hub/cifar10/data_1/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12962754.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data_hub/cifar10/data_1/cifar-10-python.tar.gz to ../../data_hub/cifar10/data_1\n",
            "Files already downloaded and verified\n",
            "[1 epoch,100]  loss:1.986396\n",
            "learn_rate:0.010000000000000\n",
            "[2 epoch,100]  loss:1.377919\n",
            "learn_rate:0.010000000000000\n",
            "[3 epoch,100]  loss:1.180463\n",
            "learn_rate:0.010000000000000\n",
            "[4 epoch,100]  loss:1.042457\n",
            "learn_rate:0.010000000000000\n",
            "[5 epoch,100]  loss:0.929018\n",
            "learn_rate:0.010000000000000\n",
            "[6 epoch,100]  loss:0.841195\n",
            "learn_rate:0.010000000000000\n",
            "[7 epoch,100]  loss:0.757213\n",
            "learn_rate:0.010000000000000\n",
            "[8 epoch,100]  loss:0.696968\n",
            "learn_rate:0.010000000000000\n",
            "[9 epoch,100]  loss:0.633942\n",
            "learn_rate:0.010000000000000\n",
            "[10 epoch,100]  loss:0.574235\n",
            "learn_rate:0.010000000000000\n",
            "[11 epoch,100]  loss:0.449523\n",
            "learn_rate:0.005000000000000\n",
            "[12 epoch,100]  loss:0.367346\n",
            "learn_rate:0.005000000000000\n",
            "[13 epoch,100]  loss:0.316708\n",
            "learn_rate:0.005000000000000\n",
            "[14 epoch,100]  loss:0.287442\n",
            "learn_rate:0.005000000000000\n",
            "[15 epoch,100]  loss:0.254513\n",
            "learn_rate:0.005000000000000\n",
            "[16 epoch,100]  loss:0.240061\n",
            "learn_rate:0.005000000000000\n",
            "[17 epoch,100]  loss:0.216981\n",
            "learn_rate:0.005000000000000\n",
            "[18 epoch,100]  loss:0.199735\n",
            "learn_rate:0.005000000000000\n",
            "[19 epoch,100]  loss:0.184522\n",
            "learn_rate:0.005000000000000\n",
            "[20 epoch,100]  loss:0.178005\n",
            "learn_rate:0.005000000000000\n",
            "[21 epoch,100]  loss:0.110794\n",
            "learn_rate:0.002500000000000\n",
            "[22 epoch,100]  loss:0.058208\n",
            "learn_rate:0.002500000000000\n",
            "[23 epoch,100]  loss:0.042699\n",
            "learn_rate:0.002500000000000\n",
            "[24 epoch,100]  loss:0.032792\n",
            "learn_rate:0.002500000000000\n",
            "[25 epoch,100]  loss:0.026553\n",
            "learn_rate:0.002500000000000\n",
            "[26 epoch,100]  loss:0.017514\n",
            "learn_rate:0.002500000000000\n",
            "[27 epoch,100]  loss:0.016174\n",
            "learn_rate:0.002500000000000\n",
            "[28 epoch,100]  loss:0.010840\n",
            "learn_rate:0.002500000000000\n",
            "[29 epoch,100]  loss:0.008930\n",
            "learn_rate:0.002500000000000\n",
            "[30 epoch,100]  loss:0.009288\n",
            "learn_rate:0.002500000000000\n",
            "[31 epoch,100]  loss:0.005999\n",
            "learn_rate:0.001250000000000\n",
            "[32 epoch,100]  loss:0.005519\n",
            "learn_rate:0.001250000000000\n",
            "[33 epoch,100]  loss:0.003596\n",
            "learn_rate:0.001250000000000\n",
            "[34 epoch,100]  loss:0.003170\n",
            "learn_rate:0.001250000000000\n",
            "[35 epoch,100]  loss:0.002741\n",
            "learn_rate:0.001250000000000\n",
            "[36 epoch,100]  loss:0.002789\n",
            "learn_rate:0.001250000000000\n",
            "[37 epoch,100]  loss:0.002646\n",
            "learn_rate:0.001250000000000\n",
            "[38 epoch,100]  loss:0.002010\n",
            "learn_rate:0.001250000000000\n",
            "[39 epoch,100]  loss:0.002613\n",
            "learn_rate:0.001250000000000\n",
            "[40 epoch,100]  loss:0.002247\n",
            "learn_rate:0.001250000000000\n",
            "[41 epoch,100]  loss:0.001985\n",
            "learn_rate:0.000625000000000\n",
            "[42 epoch,100]  loss:0.001683\n",
            "learn_rate:0.000625000000000\n",
            "[43 epoch,100]  loss:0.001752\n",
            "learn_rate:0.000625000000000\n",
            "[44 epoch,100]  loss:0.001685\n",
            "learn_rate:0.000625000000000\n",
            "[45 epoch,100]  loss:0.001758\n",
            "learn_rate:0.000625000000000\n",
            "[46 epoch,100]  loss:0.001487\n",
            "learn_rate:0.000625000000000\n",
            "[47 epoch,100]  loss:0.001563\n",
            "learn_rate:0.000625000000000\n",
            "[48 epoch,100]  loss:0.001610\n",
            "learn_rate:0.000625000000000\n",
            "[49 epoch,100]  loss:0.001409\n",
            "learn_rate:0.000625000000000\n",
            "[50 epoch,100]  loss:0.001439\n",
            "learn_rate:0.000625000000000\n",
            "[51 epoch,100]  loss:0.001312\n",
            "learn_rate:0.000312500000000\n",
            "[52 epoch,100]  loss:0.001402\n",
            "learn_rate:0.000312500000000\n",
            "[53 epoch,100]  loss:0.001447\n",
            "learn_rate:0.000312500000000\n",
            "[54 epoch,100]  loss:0.001233\n",
            "learn_rate:0.000312500000000\n",
            "[55 epoch,100]  loss:0.001257\n",
            "learn_rate:0.000312500000000\n",
            "[56 epoch,100]  loss:0.001311\n",
            "learn_rate:0.000312500000000\n",
            "[57 epoch,100]  loss:0.001257\n",
            "learn_rate:0.000312500000000\n",
            "[58 epoch,100]  loss:0.001266\n",
            "learn_rate:0.000312500000000\n",
            "[59 epoch,100]  loss:0.001183\n",
            "learn_rate:0.000312500000000\n",
            "[60 epoch,100]  loss:0.001104\n",
            "learn_rate:0.000312500000000\n",
            "[61 epoch,100]  loss:0.001175\n",
            "learn_rate:0.000156250000000\n",
            "[62 epoch,100]  loss:0.001187\n",
            "learn_rate:0.000156250000000\n",
            "[63 epoch,100]  loss:0.001228\n",
            "learn_rate:0.000156250000000\n",
            "[64 epoch,100]  loss:0.001208\n",
            "learn_rate:0.000156250000000\n",
            "[65 epoch,100]  loss:0.001236\n",
            "learn_rate:0.000156250000000\n",
            "[66 epoch,100]  loss:0.001119\n",
            "learn_rate:0.000156250000000\n",
            "[67 epoch,100]  loss:0.001225\n",
            "learn_rate:0.000156250000000\n",
            "[68 epoch,100]  loss:0.001138\n",
            "learn_rate:0.000156250000000\n",
            "[69 epoch,100]  loss:0.001132\n",
            "learn_rate:0.000156250000000\n",
            "[70 epoch,100]  loss:0.001097\n",
            "learn_rate:0.000156250000000\n",
            "time:1908.5356271266937\n",
            "=======================test=======================\n",
            "Accuracy of the network on the 10000 test images:72.03 %\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5、自定义框架ResNet-50+CIFAR-10"
      ],
      "metadata": {
        "id": "UPtw9UJeZqXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# 判断是否有GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 100 #50轮\n",
        "batch_size = 100 #50步长\n",
        "learning_rate = 0.01 #学习率0.01\n",
        "\n",
        "# 图像预处理\n",
        "transform = transforms.Compose([\n",
        "                transforms.Pad(4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 数据集下载\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../data/',\n",
        "                        train=True, \n",
        "                        transform=transform,\n",
        "                        download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../data/',\n",
        "                       train=False, \n",
        "                       transform=transforms.ToTensor())\n",
        "\n",
        "# 数据载入\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                       batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# # 构建ResNet-34模型\n",
        "# def resnet34(num_classes=1000, include_top=True):\n",
        "#     return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "# # \n",
        "\n",
        "# 构建ResNet-50模型\n",
        "def resnet50(num_classes=1000, include_top=True):\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "#模型,优化器\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "model = resnet50().to(device)\n",
        "\n",
        "\n",
        "# 损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 更新学习率\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# 训练数据集\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # 延迟学习率\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)\n",
        "\n",
        "    # 测试网络模型\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# S将模型保存\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "KQWjige_D37x",
        "outputId": "a13e55d1-c365-439d-cb2f-f78b48d35855"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 15718003.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/cifar-10-python.tar.gz to ../data/\n",
            "Epoch [1/100], Step [100/500] Loss: 4.7510\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d161352b1e97>\u001b[0m in \u001b[0;36m<cell line: 198>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-d161352b1e97>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VlSgZvm770IU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}